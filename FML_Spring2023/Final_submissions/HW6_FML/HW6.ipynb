{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89295c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install box2d-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3377387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyvirtualdisplay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xvfbwrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc47ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'gym[atari]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c89103",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym==0.21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95795bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2049f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install box2d-py\n",
    "!pip install gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc57f785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyglet\n",
      "  Downloading pyglet-2.0.7-py3-none-any.whl (841 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.0/841.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyglet\n",
      "Successfully installed pyglet-2.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f00132e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import gym\n",
    "from gym.envs.box2d.lunar_lander import LunarLanderContinuous\n",
    "\n",
    "# Define the policy network\n",
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc = torch.nn.Linear(8, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "# Function to preprocess the observation\n",
    "def preprocess(observation):\n",
    "    return torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "# Create the environment\n",
    "env = LunarLanderContinuous()\n",
    "\n",
    "# Create the policy network\n",
    "policy = Policy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92cddaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Episode: 100\n",
      "Episode: 200\n",
      "Episode: 300\n",
      "Episode: 400\n",
      "Episode: 500\n",
      "Episode: 600\n",
      "Episode: 700\n",
      "Episode: 800\n",
      "Episode: 900\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "# Define policy network\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, output_size)\n",
    "\n",
    "        self.action_var = torch.full((output_size,), 0.1)\n",
    "        self.action_var.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        state = state.unsqueeze(0)  # Add batch dimension\n",
    "        action_mean = self.forward(state)\n",
    "        cov_matrix = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "        action_distribution = Normal(action_mean, cov_matrix)\n",
    "        action = action_distribution.sample()\n",
    "        action_log_prob = action_distribution.log_prob(action)\n",
    "        return action.detach().numpy().flatten(), action_log_prob.detach()\n",
    "\n",
    "# Define function to calculate discounted rewards\n",
    "def calculate_discounted_rewards(rewards, gamma=0.99):\n",
    "    discounted_rewards = []\n",
    "    running_reward = 0\n",
    "    for reward in reversed(rewards):\n",
    "        running_reward = reward + gamma * running_reward\n",
    "        discounted_rewards.insert(0, running_reward)\n",
    "    return discounted_rewards\n",
    "\n",
    "def update_policy(optimizer, action_log_probs, rewards):\n",
    "    discounted_rewards = calculate_discounted_rewards(rewards)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)  # Convert to tensor\n",
    "    discounted_rewards -= torch.mean(discounted_rewards)\n",
    "    discounted_rewards /= torch.std(discounted_rewards)\n",
    "\n",
    "    policy_loss = []\n",
    "    for log_prob, reward in zip(action_log_probs, discounted_rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    policy_loss = torch.stack(policy_loss).sum().detach()  # Detach the tensor and sum\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.requires_grad = True  # Set requires_grad flag to True\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Set up environment\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "env.seed(0)\n",
    "\n",
    "# Set up policy network\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.shape[0]\n",
    "policy = Policy(input_size, output_size)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "# Train policy network\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    action_log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        action, action_log_prob = policy.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        action_log_probs.append(action_log_prob)\n",
    "        state = next_state\n",
    "\n",
    "    update_policy(optimizer, action_log_probs, rewards)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode:\", episode)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24a63eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Episode reward: -1.3131929686165242\n",
      "Running mean reward: -1.3131929686165242\n",
      "Episode: 100\n",
      "Episode reward: -128.6321633058997\n",
      "Running mean reward: -2.586382671989356\n",
      "Episode: 200\n",
      "Episode reward: -127.75877968579552\n",
      "Running mean reward: -3.8381066421274177\n",
      "Episode: 300\n",
      "Episode reward: -128.96515185037305\n",
      "Running mean reward: -5.089377094209874\n",
      "Episode: 400\n",
      "Episode reward: -128.9889248535108\n",
      "Running mean reward: -6.328372571802884\n",
      "Episode: 500\n",
      "Episode reward: -128.6306236862712\n",
      "Running mean reward: -7.551395082947567\n",
      "Episode: 600\n",
      "Episode reward: -129.0310552587112\n",
      "Running mean reward: -8.766191684705204\n",
      "Episode: 700\n",
      "Episode reward: -129.43150324220554\n",
      "Running mean reward: -9.972844800280207\n",
      "Episode: 800\n",
      "Episode reward: -128.16172812964814\n",
      "Running mean reward: -11.154733633573887\n",
      "Episode: 900\n",
      "Episode reward: -128.38184864402135\n",
      "Running mean reward: -12.327004783678362\n",
      "Episode: 1000\n",
      "Episode reward: -129.32334614243194\n",
      "Running mean reward: -13.496968197265899\n",
      "Episode: 1100\n",
      "Episode reward: -127.93487122872585\n",
      "Running mean reward: -14.641347227580498\n",
      "Episode: 1200\n",
      "Episode reward: -128.89013532257874\n",
      "Running mean reward: -15.78383510853048\n",
      "Episode: 1300\n",
      "Episode reward: -129.01154534981143\n",
      "Running mean reward: -16.91611221094329\n",
      "Episode: 1400\n",
      "Episode reward: -128.79415815718306\n",
      "Running mean reward: -18.03489267040569\n",
      "Episode: 1500\n",
      "Episode reward: -128.950040616441\n",
      "Running mean reward: -19.144044149866044\n",
      "Episode: 1600\n",
      "Episode reward: -129.18041525898795\n",
      "Running mean reward: -20.244407860957264\n",
      "Episode: 1700\n",
      "Episode reward: -128.7716140843998\n",
      "Running mean reward: -21.329679923191687\n",
      "Episode: 1800\n",
      "Episode reward: -129.80086988188197\n",
      "Running mean reward: -22.41439182277859\n",
      "Episode: 1900\n",
      "Episode reward: -128.87567356476634\n",
      "Running mean reward: -23.479004640198468\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Define policy network\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, output_size)\n",
    "\n",
    "        self.action_var = torch.full((output_size,), 0.1)\n",
    "        self.action_var.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        state = state.unsqueeze(0)  # Add batch dimension\n",
    "        action_mean = self.forward(state)\n",
    "        cov_matrix = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "        action_distribution = Normal(action_mean, cov_matrix)\n",
    "        action = action_distribution.sample()\n",
    "        action_log_prob = action_distribution.log_prob(action)\n",
    "        return action.detach().numpy().flatten(), action_log_prob.detach()\n",
    "\n",
    "# Define function to calculate discounted rewards\n",
    "def calculate_discounted_rewards(rewards, gamma=0.99):\n",
    "    discounted_rewards = []\n",
    "    running_reward = 0\n",
    "    for reward in reversed(rewards):\n",
    "        running_reward = reward + gamma * running_reward\n",
    "        discounted_rewards.insert(0, running_reward)\n",
    "    return discounted_rewards\n",
    "\n",
    "def update_policy(optimizer, action_log_probs, rewards):\n",
    "    discounted_rewards = calculate_discounted_rewards(rewards)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)  # Convert to tensor\n",
    "    discounted_rewards -= torch.mean(discounted_rewards)\n",
    "    discounted_rewards /= torch.std(discounted_rewards)\n",
    "\n",
    "    policy_loss = []\n",
    "    for log_prob, reward in zip(action_log_probs, discounted_rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    policy_loss = torch.stack(policy_loss).sum().detach()  # Detach the tensor and sum\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.requires_grad = True  # Set requires_grad flag to True\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Set up environment\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "env.seed(0)\n",
    "\n",
    "# Set up policy network\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.shape[0]\n",
    "policy = Policy(input_size, output_size)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "# Train policy network\n",
    "num_episodes = 1000\n",
    "reward_sum = 0\n",
    "running_reward = None\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    action_log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        action, action_log_prob = policy.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        action_log_probs.append(action_log_prob)\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "\n",
    "    update_policy(optimizer, action_log_probs, rewards)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode:\", episode)\n",
    "        print(\"Episode reward:\", reward_sum)\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print(\"Running mean reward:\", running_reward)\n",
    "        reward_sum = 0\n",
    "        if episode % 500 == 0:\n",
    "            pickle.dump(policy, open(\"policy_model.p\", \"wb\"))\n",
    "\n",
    "# Continue training policy network\n",
    "for episode in range(num_episodes, 2*num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    action_log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        action, action_log_prob = policy.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        action_log_probs.append(action_log_prob)\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "\n",
    "    update_policy(optimizer, action_log_probs, rewards)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode:\", episode)\n",
    "        print(\"Episode reward:\", reward_sum)\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print(\"Running mean reward:\", running_reward)\n",
    "        reward_sum = 0\n",
    "        if episode % 500 == 0:\n",
    "            pickle.dump(policy, open(\"policy_model.p\", \"wb\"))\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab2579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Episode reward: -4.553510936759021\n",
      "Running mean reward: -4.553510936759021\n",
      "Episode: 100\n",
      "Episode reward: -460.69996009474636\n",
      "Running mean reward: -9.114975428338894\n",
      "Episode: 200\n",
      "Episode reward: -461.82510607477656\n",
      "Running mean reward: -13.642076734803272\n",
      "Episode: 300\n",
      "Episode reward: -458.5810162471544\n",
      "Running mean reward: -18.091466129926783\n",
      "Episode: 400\n",
      "Episode reward: -457.75896675458176\n",
      "Running mean reward: -22.488141136173333\n",
      "Episode: 500\n",
      "Episode reward: -460.61234778905435\n",
      "Running mean reward: -26.869383202702146\n",
      "Episode: 600\n",
      "Episode reward: -460.467127930874\n",
      "Running mean reward: -31.205360649983863\n",
      "Episode: 700\n",
      "Episode reward: -461.0713728803938\n",
      "Running mean reward: -35.504020772287966\n",
      "Episode: 800\n",
      "Episode reward: -457.90230256727284\n",
      "Running mean reward: -39.72800359023781\n",
      "Episode: 900\n",
      "Episode reward: -459.6691495698153\n",
      "Running mean reward: -43.92741505003359\n",
      "Episode: 1000\n",
      "Episode reward: -458.3076548477918\n",
      "Running mean reward: -48.07121744801117\n",
      "Episode: 1100\n",
      "Episode reward: -458.6006161516771\n",
      "Running mean reward: -52.17651143504783\n",
      "Episode: 1200\n",
      "Episode reward: -460.5987656120445\n",
      "Running mean reward: -56.2607339768178\n",
      "Episode: 1300\n",
      "Episode reward: -457.933157249899\n",
      "Running mean reward: -60.27745820954861\n",
      "Episode: 1400\n",
      "Episode reward: -458.5725888914718\n",
      "Running mean reward: -64.26040951636784\n",
      "Episode: 1500\n",
      "Episode reward: -460.81866997131516\n",
      "Running mean reward: -68.22599212091731\n",
      "Episode: 1600\n",
      "Episode reward: -458.1398618214839\n",
      "Running mean reward: -72.12513081792297\n",
      "Episode: 1700\n",
      "Episode reward: -461.4750905806873\n",
      "Running mean reward: -76.0186304155506\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Define policy network\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, output_size)\n",
    "\n",
    "        self.action_var = torch.full((output_size,), 0.1)\n",
    "        self.action_var.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        state = state.unsqueeze(0)  # Add batch dimension\n",
    "        action_mean = self.forward(state)\n",
    "        cov_matrix = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "        action_distribution = Normal(action_mean, cov_matrix)\n",
    "        action = action_distribution.sample()\n",
    "        action_log_prob = action_distribution.log_prob(action)\n",
    "        return action.detach().numpy().flatten(), action_log_prob.detach()\n",
    "\n",
    "# Define function to calculate discounted rewards\n",
    "def calculate_discounted_rewards(rewards, gamma=0.99):\n",
    "    discounted_rewards = []\n",
    "    running_reward = 0\n",
    "    for reward in reversed(rewards):\n",
    "        running_reward = reward + gamma * running_reward\n",
    "        discounted_rewards.insert(0, running_reward)\n",
    "    return discounted_rewards\n",
    "\n",
    "def update_policy(optimizer, action_log_probs, rewards):\n",
    "    discounted_rewards = calculate_discounted_rewards(rewards)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)  # Convert to tensor\n",
    "    discounted_rewards -= torch.mean(discounted_rewards)\n",
    "    discounted_rewards /= torch.std(discounted_rewards)\n",
    "\n",
    "    policy_loss = []\n",
    "    for log_prob, reward in zip(action_log_probs, discounted_rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    policy_loss = torch.stack(policy_loss).sum().detach()  # Detach the tensor and sum\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.requires_grad = True  # Set requires_grad flag to True\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Set up environment\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "env.seed(0)\n",
    "\n",
    "# Set up policy network\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.shape[0]\n",
    "policy = Policy(input_size, output_size)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "# Train policy network\n",
    "num_episodes = 1000\n",
    "reward_sum = 0\n",
    "running_reward = None\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    action_log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        action, action_log_prob = policy.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        action_log_probs.append(action_log_prob)\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "\n",
    "    update_policy(optimizer, action_log_probs, rewards)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode:\", episode)\n",
    "        print(\"Episode reward:\", reward_sum)\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print(\"Running mean reward:\", running_reward)\n",
    "        reward_sum = 0\n",
    "        if episode % 500 == 0:                         \n",
    "            pickle.dump(policy, open(\"policy_model.p\", \"wb\"))\n",
    "\n",
    "# Continue training policy network\n",
    "for episode in range(num_episodes, 2 * num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    action_log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        action, action_log_prob = policy.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        action_log_probs.append(action_log_prob)\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "\n",
    "    update_policy(optimizer, action_log_probs, rewards)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode:\", episode)\n",
    "        print(\"Episode reward:\", reward_sum)\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print(\"Running mean reward:\", running_reward)\n",
    "        reward_sum = 0\n",
    "        if episode % 500 == 0:\n",
    "            pickle.dump(policy, open(\"policy_model.p\", \"wb\"))\n",
    "\n",
    "env.close()\n",
    "\n",
    "def play_game(env, model):\n",
    "    observation, _ = env.reset()\n",
    "\n",
    "    frames = []\n",
    "    cumulated_reward = 0\n",
    "\n",
    "    prev_x = None  # used in computing the difference frame\n",
    "\n",
    "    for t in range(1000):\n",
    "        frames.append(env.render())\n",
    "        action, prev_x = model_step(model, observation, prev_x)\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        cumulated_reward += reward\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t + 1, cumulated_reward))\n",
    "            break\n",
    "    print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n",
    "    env.close()\n",
    "    return frames\n",
    "\n",
    "def model_step(model, observation, prev_x):\n",
    "    processed_observation = preprocess(observation)\n",
    "    x = processed_observation - prev_x if prev_x is not None else np.zeros(processed_observation.shape)\n",
    "    prev_x = processed_observation\n",
    "    action, _ = model.act(x)\n",
    "    return action, prev_x\n",
    "\n",
    "def preprocess(observation):\n",
    "    # Modify the preprocessing step according to your needs\n",
    "    return observation\n",
    "\n",
    "# Play the game using the trained policy\n",
    "model = pickle.load(open(\"policy_model.p\", \"rb\"))\n",
    "frames = play_game(env, model)\n",
    "\n",
    "# Display the frames as a video\n",
    "for frame in frames:\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df143359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        plt.imshow(frame)\n",
    "        plt.axis('off')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "# Play the game using the trained policy\n",
    "model = pickle.load(open(\"policy_model.p\", \"rb\"))\n",
    "frames = play_game(env, model)\n",
    "\n",
    "# Display the frames as a video\n",
    "print_frames(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453ae13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Episode reward: -2.28883864983427\n",
      "Running mean reward: -2.28883864983427\n",
      "Episode: 100\n",
      "Episode reward: -221.85866980624292\n",
      "Running mean reward: -4.484536961398357\n",
      "Episode: 200\n",
      "Episode reward: -222.75740610790533\n",
      "Running mean reward: -6.667265652863426\n",
      "Episode: 300\n",
      "Episode reward: -221.00578239261446\n",
      "Running mean reward: -8.810650820260935\n",
      "Episode: 400\n",
      "Episode reward: -222.60338787295439\n",
      "Running mean reward: -10.94857819078787\n",
      "Episode: 500\n",
      "Episode reward: -221.53630490100628\n",
      "Running mean reward: -13.054455457890054\n",
      "Episode: 600\n",
      "Episode reward: -222.29873518014097\n",
      "Running mean reward: -15.146898255112562\n",
      "Episode: 700\n",
      "Episode reward: -224.13186182136053\n",
      "Running mean reward: -17.236747890775042\n",
      "Episode: 800\n",
      "Episode reward: -222.246151551902\n",
      "Running mean reward: -19.28684192738631\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Define policy network\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.fc2 = nn.Linear(32, output_size)\n",
    "\n",
    "        self.action_var = torch.full((output_size,), 0.1)\n",
    "        self.action_var.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        state = state.unsqueeze(0)  # Add batch dimension\n",
    "        action_mean = self.forward(state)\n",
    "        cov_matrix = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "        action_distribution = Normal(action_mean, cov_matrix)\n",
    "        action = action_distribution.sample()\n",
    "        action_log_prob = action_distribution.log_prob(action)\n",
    "        return action.detach().numpy().flatten(), action_log_prob.detach()\n",
    "\n",
    "# Define function to calculate discounted rewards\n",
    "def calculate_discounted_rewards(rewards, gamma=0.99):\n",
    "    discounted_rewards = []\n",
    "    running_reward = 0\n",
    "    for reward in reversed(rewards):\n",
    "        running_reward = reward + gamma * running_reward\n",
    "        discounted_rewards.insert(0, running_reward)\n",
    "    return discounted_rewards\n",
    "\n",
    "def update_policy(optimizer, action_log_probs, rewards):\n",
    "    discounted_rewards = calculate_discounted_rewards(rewards)\n",
    "    discounted_rewards = torch.tensor(discounted_rewards)  # Convert to tensor\n",
    "    discounted_rewards -= torch.mean(discounted_rewards)\n",
    "    discounted_rewards /= torch.std(discounted_rewards)\n",
    "\n",
    "    policy_loss = []\n",
    "    for log_prob, reward in zip(action_log_probs, discounted_rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "    policy_loss = torch.stack(policy_loss).sum().detach()  # Detach the tensor and sum\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss.requires_grad = True  # Set requires_grad flag to True\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# # Set up environment\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "env.seed(0)\n",
    "\n",
    "# Set up policy network\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.shape[0]\n",
    "policy = Policy(input_size, output_size)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "# Train policy network\n",
    "num_episodes = 1000\n",
    "reward_sum = 0\n",
    "running_reward = None\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    action_log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        action, action_log_prob = policy.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        action_log_probs.append(action_log_prob)\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "\n",
    "    update_policy(optimizer, action_log_probs, rewards)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode:\", episode)\n",
    "        print(\"Episode reward:\", reward_sum)\n",
    "        running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "        print(\"Running mean reward:\", running_reward)\n",
    "        reward_sum = 0\n",
    "        episode_rewards.append(running_reward)\n",
    "\n",
    "# Train policy network\n",
    "num_episodes = 1000\n",
    "reward_sum = 0\n",
    "running_reward = 0\n",
    "episode_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    rewards = []\n",
    "    action_log_probs = []\n",
    "\n",
    "    while not done:\n",
    "        action, action_log_prob = policy.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        action_log_probs.append(action_log_prob)\n",
    "        state = next_state\n",
    "        reward_sum += reward\n",
    "\n",
    "    update_policy(optimizer, action_log_probs, rewards)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode:\", episode)\n",
    "        print(\"Episode reward:\", reward_sum)\n",
    "        running_reward = reward_sum / 100  # Calculate running mean reward\n",
    "        print(\"Running mean reward:\", running_reward)\n",
    "        reward_sum = 0\n",
    "        episode_rewards.append(running_reward)\n",
    "\n",
    "# Plot rewards\n",
    "plt.plot(episode_rewards)\n",
    "plt.title('Running Mean Reward over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Running Mean Reward')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Plot\n",
    "# # Plot rewards\n",
    "# plt.plot(episode_rewards)\n",
    "# plt.title('Total Reward over Episodes')\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Total Reward')\n",
    "# plt.show()\n",
    "\n",
    "# # Continue training policy network\n",
    "# for episode in range(num_episodes, 2*num_episodes):\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     rewards = []\n",
    "#     action_log_probs = []\n",
    "\n",
    "#     while not done:\n",
    "#         action, action_log_prob = policy.act(state)\n",
    "#         next_state, reward, done, _ = env.step(action)\n",
    "#         rewards.append(reward)\n",
    "#         action_log_probs.append(action_log_prob)\n",
    "#         state = next_state\n",
    "#         reward_sum += reward\n",
    "\n",
    "#     update_policy(optimizer, action_log_probs, rewards)\n",
    "\n",
    "#     if episode % 100 == 0:\n",
    "#         print(\"Episode:\", episode)\n",
    "#         print(\"Episode reward:\", reward_sum)\n",
    "#         running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "#         print(\"Running mean reward:\", running_reward)\n",
    "#         reward_sum = 0\n",
    "#         episode_rewards.append(running_reward)\n",
    "\n",
    "# # Plot rewards\n",
    "# plt.plot(episode_rewards)\n",
    "# plt.title('Total Reward over Episodes')\n",
    "# plt.xlabel('Episode')\n",
    "# plt.ylabel('Total Reward')\n",
    "# plt.show()\n",
    "\n",
    "# env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
